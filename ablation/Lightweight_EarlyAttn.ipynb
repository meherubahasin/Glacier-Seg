{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c23baca",
      "metadata": {
        "id": "0c23baca",
        "outputId": "9dc3d06c-1352-4973-b0bc-6d0afe376456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]\n",
            "PyTorch: 2.8.0+cu129\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA GeForce RTX 3070 Ti\n",
            "mamba_ssm found: True\n",
            "mamba_model found: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, sys, torch\n",
        "print('Python:', sys.version)\n",
        "print('PyTorch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('CUDA device:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Check presence of mamba_ssm (optional) and mamba_model (required)\n",
        "try:\n",
        "    import importlib.util\n",
        "    spec = importlib.util.find_spec('mamba_ssm')\n",
        "    print('mamba_ssm found:', spec is not None)\n",
        "except Exception as e:\n",
        "    print('mamba_ssm check error:', e)\n",
        "\n",
        "try:\n",
        "    spec2 = importlib.util.find_spec('mamba_model')\n",
        "    print('mamba_model found:', spec2 is not None)\n",
        "except Exception as e:\n",
        "    print('mamba_model check error:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca7f0445",
      "metadata": {
        "id": "ca7f0445"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Optional RMSNorm from mamba_ssm; fall back to LayerNorm if missing ---\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layer_norm import RMSNorm\n",
        "except Exception:\n",
        "    RMSNorm = None\n",
        "\n",
        "# --- REQUIRED: your local Mamba implementation exported from mamba_model.py ---\n",
        "from mamba_model import Mamba\n",
        "\n",
        "\n",
        "# ------------------------- Utils -------------------------\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob: float = 0.):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.drop_prob == 0. or not self.training:\n",
        "            return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()\n",
        "        return x.div(keep_prob) * random_tensor\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ------------------------- Involution stem -------------------------\n",
        "class Involution2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Involution: location-specific, channel-agnostic kernel.\n",
        "    Paper: \"Involution: Inverting the Inherence of Convolution for Visual Recognition\" (CVPR 2021)\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, kernel_size=3, stride=1, reduction_ratio=4, groups=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.groups = groups\n",
        "\n",
        "        hidden = max(channels // reduction_ratio, 1)\n",
        "        self.reduce = nn.Conv2d(channels, hidden, 1)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        self.span = nn.Conv2d(hidden, groups * (kernel_size * kernel_size), 1)\n",
        "        self.sigma = nn.AvgPool2d(kernel_size=stride, stride=stride) if stride > 1 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        k, g = self.kernel_size, self.groups\n",
        "        assert C % g == 0, f\"Channels ({C}) must be divisible by groups ({g}).\"\n",
        "\n",
        "        # 1) Generate spatially-varying kernels on possibly downsampled features\n",
        "        x_k = self.sigma(x)                                   # (B, C, H', W')\n",
        "        K = self.span(self.act(self.reduce(x_k)))             # (B, g*k*k, H', W')\n",
        "        Hout, Wout = K.shape[2], K.shape[3]\n",
        "        K = K.view(B, g, k * k, Hout, Wout)                   # (B, g, k*k, H', W')\n",
        "\n",
        "        # 2) Unfold patches on original x with stride/padding so H',W' align\n",
        "        patches = F.unfold(x, kernel_size=k, dilation=1, padding=k // 2, stride=self.stride)\n",
        "        patches = patches.view(B, g, C // g, k * k, Hout, Wout)  # (B, g, Cg, k*k, H', W')\n",
        "\n",
        "        # 3) Channel-agnostic weighted sum within group\n",
        "        out = (patches * K.unsqueeze(2)).sum(dim=3)           # (B, g, Cg, H', W')\n",
        "        out = out.view(B, C, Hout, Wout)\n",
        "        return out\n",
        "\n",
        "\n",
        "class OverlapPatchEmbedInvo(nn.Module):\n",
        "    \"\"\"Involution downsample + 1x1 projection to embed_dim.\"\"\"\n",
        "    def __init__(self, in_chans=3, embed_dim=32, kernel_size=3, stride=2, padding=1,\n",
        "                 reduction_ratio=4, groups=1):\n",
        "        super().__init__()\n",
        "        self.invo = Involution2D(in_chans, kernel_size=kernel_size, stride=stride,\n",
        "                                 reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=1, stride=1, padding=0)\n",
        "        self.norm = nn.BatchNorm2d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.invo(x)     # downsample + spatially-varying mixing\n",
        "        x = self.proj(x)     # channel set to embed_dim\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ------------------------- Conv stem (fast option) -------------------------\n",
        "class OverlapPatchEmbedConv(nn.Module):\n",
        "    \"\"\"Standard conv downsample + BN + ReLU.\"\"\"\n",
        "    def __init__(self, in_chans=3, embed_dim=32, kernel_size=3, stride=2, padding=1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, embed_dim, kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(embed_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "# ------------------------- Mixer Block (Mamba) -------------------------\n",
        "class MambaBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Mamba SSM mixer on (B, N, C) with PreNorm + MLP and residuals.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0, drop_path=0.0, d_state=16, use_rmsnorm=True):\n",
        "        super().__init__()\n",
        "        Norm = RMSNorm if (use_rmsnorm and RMSNorm is not None) else nn.LayerNorm\n",
        "\n",
        "        self.norm1 = Norm(dim)\n",
        "        self.mamba = Mamba(d_model=dim, d_state=d_state)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        self.norm2 = Norm(dim)\n",
        "        hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=hidden_dim, act_layer=nn.GELU, drop=drop)\n",
        "\n",
        "    def forward(self, x, H=None, W=None):\n",
        "        # x: (B, N, C)\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.mamba(x)\n",
        "        x = self.drop_path(x) + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop_path(x) + shortcut\n",
        "        return x\n",
        "\n",
        "\n",
        "# ------------------------- MiT Stage (stem + Mamba blocks) -------------------------\n",
        "class MiTStage(nn.Module):\n",
        "    def __init__(self, in_chs, embed_dim, depth, drop_path_rates=None, use_involution=True):\n",
        "        super().__init__()\n",
        "        # choose stem type\n",
        "        if use_involution:\n",
        "            self.patch_embed = OverlapPatchEmbedInvo(\n",
        "                in_chans=in_chs, embed_dim=embed_dim, kernel_size=3, stride=2, padding=1,\n",
        "                reduction_ratio=4, groups=1\n",
        "            )\n",
        "        else:\n",
        "            self.patch_embed = OverlapPatchEmbedConv(\n",
        "                in_chans=in_chs, embed_dim=embed_dim, kernel_size=3, stride=2, padding=1\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            MambaBlock(\n",
        "                dim=embed_dim,\n",
        "                mlp_ratio=4.0,\n",
        "                drop=0.0,\n",
        "                drop_path=(drop_path_rates[i] if drop_path_rates is not None else 0.0),\n",
        "                d_state=16,\n",
        "                use_rmsnorm=True\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)              # (B, C, H', W')\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)     # (B, N, C)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, H, W)\n",
        "        x = x.transpose(1, 2).reshape(B, C, H, W)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ------------------------- Mix Vision Transformer (backbone) -------------------------\n",
        "class MixVisionTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Full MiT backbone (4 stages) returning multi-scale features:\n",
        "        [S1, S2, S3, S4] with strides [2, 4, 8, 16] wrt input.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_chans: int = 3,\n",
        "                 embed_dims = [32, 64, 160, 256],\n",
        "                 depths    = [2, 2, 2, 2],\n",
        "                 drop_path_rate: float = 0.0,\n",
        "                 use_invo_stages = (True, True, False, False)):\n",
        "        super().__init__()\n",
        "        assert len(embed_dims) == 4 and len(depths) == 4 and len(use_invo_stages) == 4\n",
        "\n",
        "        # Stochastic depth schedule across all blocks\n",
        "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        cur = 0\n",
        "\n",
        "        self.stages = nn.ModuleList()\n",
        "        in_c = in_chans\n",
        "        for i in range(4):\n",
        "            d = depths[i]\n",
        "            dpr_slice = dp_rates[cur:cur + d]\n",
        "            cur += d\n",
        "\n",
        "            self.stages.append(\n",
        "                MiTStage(\n",
        "                    in_chs=in_c,\n",
        "                    embed_dim=embed_dims[i],\n",
        "                    depth=d,\n",
        "                    drop_path_rates=dpr_slice,\n",
        "                    use_involution=use_invo_stages[i],\n",
        "                )\n",
        "            )\n",
        "            in_c = embed_dims[i]\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "            features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "# ------------------------------- Decoder --------------------------------\n",
        "class SegFormerHead(nn.Module):\n",
        "    def __init__(self, in_channels: List[int], embed_dim=128, num_classes=19):\n",
        "        super().__init__()\n",
        "        self.proj_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(in_ch, embed_dim, 1, bias=False),\n",
        "                nn.BatchNorm2d(embed_dim),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "            for in_ch in in_channels\n",
        "        ])\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim * len(in_channels), embed_dim, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(embed_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(embed_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.classifier = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, features: List[torch.Tensor]):\n",
        "        # Upsample all features to the highest spatial resolution (stage 1)\n",
        "        target_h, target_w = features[0].shape[2:]\n",
        "        proj = []\n",
        "        for i, feat in enumerate(features):\n",
        "            x = self.proj_layers[i](feat)\n",
        "            if x.shape[2:] != (target_h, target_w):\n",
        "                x = F.interpolate(x, size=(target_h, target_w), mode='bilinear', align_corners=False)\n",
        "            proj.append(x)\n",
        "        x = torch.cat(proj, dim=1)\n",
        "        x = self.fuse(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# -------------------------- Full SegFormer Model -------------------------\n",
        "class SegFormer(nn.Module):\n",
        "    def __init__(self, num_classes=19, variant='mit_b0', pretrained=False,\n",
        "                 drop_path_rate=0.1, in_chans=3, use_invo_stages=(True, True, False, False),\n",
        "                 embed_dims=None, depths=None): # Added embed_dims and depths\n",
        "        super().__init__()\n",
        "        variants = {\n",
        "            'mit_b0': dict(embed_dims=[16, 32, 64, 128], depths=[1, 1, 1, 1]),\n",
        "            'mit_b1': dict(embed_dims=[64, 128, 320, 512], depths=[2, 2, 2, 2]),\n",
        "            'mit_b2': dict(embed_dims=[64, 128, 320, 512], depths=[3, 4, 6, 3]),\n",
        "            'mit_b3': dict(embed_dims=[64, 128, 320, 512], depths=[3, 4, 18, 3]),\n",
        "            'mit_b4': dict(embed_dims=[64, 128, 320, 512], depths=[3, 8, 27, 3]),\n",
        "            'mit_b5': dict(embed_dims=[64, 128, 320, 512], depths=[3, 6, 40, 3]),\n",
        "        }\n",
        "\n",
        "        if variant in variants:\n",
        "            cfg = variants[variant]\n",
        "            embed_dims = embed_dims or cfg['embed_dims']\n",
        "            depths = depths or cfg['depths']\n",
        "        elif embed_dims is None or depths is None:\n",
        "             raise ValueError(f'Unknown variant: {variant} and embed_dims or depths not provided')\n",
        "\n",
        "\n",
        "        self.backbone = MixVisionTransformer(\n",
        "            in_chans=in_chans,\n",
        "            embed_dims=embed_dims, # Pass embed_dims\n",
        "            depths=depths, # Pass depths\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            use_invo_stages=use_invo_stages\n",
        "        )\n",
        "        self.decoder = SegFormerHead(in_channels=embed_dims, embed_dim=128, num_classes=num_classes) # Pass embed_dims\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        if pretrained:\n",
        "            print('pretrained=True selected but no loader is implemented in this script.')\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if getattr(m, \"bias\", None) is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if getattr(m, \"bias\", None) is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.ones_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = x.shape[2], x.shape[3]\n",
        "        feats = self.backbone(x)                       # [S1,S2,S3,S4]\n",
        "        out = self.decoder(feats)                      # logits at S1 resolution\n",
        "        out = F.interpolate(out, size=(H, W), mode='bilinear', align_corners=False)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd3d885",
      "metadata": {
        "id": "0bd3d885"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1369e8a1",
      "metadata": {
        "id": "1369e8a1"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "PATCH_SIZE = 256\n",
        "TARGET_SIZE = (256, 256)\n",
        "LABEL_KEY = 'outlines'\n",
        "MODALITIES_3 = ['dem', 'optical', 'bright_dark_outlines']\n",
        "CHANNEL_INFO_3 = {'dem': 2, 'optical': 6, 'bright_dark_outlines': 3}\n",
        "\n",
        "def normalize(arr):\n",
        "    arr = arr.astype(np.float32)\n",
        "    return (arr - arr.mean()) / (arr.std() + 1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2e84fe",
      "metadata": {
        "id": "0a2e84fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize(arr):\n",
        "    arr = arr.astype(np.float32)\n",
        "    return (arr - arr.mean()) / (arr.std() + 1e-5)\n",
        "\n",
        "def augment_patch(img, label):\n",
        "    if random.random() < 0.5:\n",
        "        img = np.flip(img, axis=0)\n",
        "        label = np.flip(label, axis=0)\n",
        "    if random.random() < 0.5:\n",
        "        img = np.flip(img, axis=1)\n",
        "        label = np.flip(label, axis=1)\n",
        "    return img, label\n",
        "\n",
        "class GlacierHDF5PatchDataset3(torch.utils.data.Dataset):\n",
        "    def __init__(self, hdf5_file_path, patch_size=PATCH_SIZE, length=2000):\n",
        "        self.hdf5 = h5py.File(hdf5_file_path, 'r')\n",
        "        self.tiles = [name for name in self.hdf5.keys() if all(m in self.hdf5[name] for m in MODALITIES_3)]\n",
        "        self.patch_size = patch_size\n",
        "        self.length = length\n",
        "        self.selected_indices = [self._get_random_patch_coords() for _ in range(length)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def _get_random_patch_coords(self):\n",
        "        tile_name = random.choice(self.tiles)\n",
        "        tile = self.hdf5[tile_name]\n",
        "        h, w = tile[MODALITIES_3[0]].shape[:2]\n",
        "        y = random.randint(0, h - self.patch_size)\n",
        "        x = random.randint(0, w - self.patch_size)\n",
        "        return tile_name, y, x\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tile_name, y, x = self.selected_indices[idx]\n",
        "        tile = self.hdf5[tile_name]\n",
        "\n",
        "        dem = normalize(tile['dem'][y:y+self.patch_size, x:x+self.patch_size, :])\n",
        "        opt = normalize(tile['optical'][y:y+self.patch_size, x:x+self.patch_size, :])\n",
        "        bd  = tile['bright_dark_outlines'][y:y+self.patch_size, x:x+self.patch_size, :].astype(np.float32)\n",
        "        if bd.max() > 1:  # scale to [0,1]\n",
        "            bd = bd / 255.0\n",
        "\n",
        "        full_patch = np.concatenate([dem, opt, bd], axis=2)\n",
        "        label = tile['outlines'][y:y+self.patch_size, x:x+self.patch_size]\n",
        "        if label.ndim == 3:\n",
        "            label = label[:, :, 0]\n",
        "\n",
        "        aug_patch, aug_label = augment_patch(full_patch.copy(), label.copy())\n",
        "        orig_tensor = torch.tensor(np.ascontiguousarray(full_patch)).permute(2,0,1).float()\n",
        "        aug_tensor  = torch.tensor(np.ascontiguousarray(aug_patch)).permute(2,0,1).float()\n",
        "\n",
        "        return orig_tensor, aug_tensor\n",
        "\n",
        "    def close(self):\n",
        "        self.hdf5.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad3f1ef3",
      "metadata": {
        "id": "ad3f1ef3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    jaccard_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "def segmentation_metrics(y_true, y_pred, num_classes=3):\n",
        "    # Flatten arrays\n",
        "    y_true = y_true.ravel()\n",
        "    y_pred = y_pred.ravel()\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
        "\n",
        "    # Pixel accuracy\n",
        "    pixel_acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # IoU (Jaccard Index)\n",
        "    per_class_iou = jaccard_score(y_true, y_pred, average=None, labels=list(range(num_classes)))\n",
        "    mean_iou = jaccard_score(y_true, y_pred, average=\"macro\", labels=list(range(num_classes)))\n",
        "\n",
        "    # Dice (F1-score)\n",
        "    per_class_dice = f1_score(y_true, y_pred, average=None, labels=list(range(num_classes)))\n",
        "    mean_dice = f1_score(y_true, y_pred, average=\"macro\", labels=list(range(num_classes)))\n",
        "\n",
        "    # Precision & Recall\n",
        "    precision = precision_score(y_true, y_pred, average=\"macro\", labels=list(range(num_classes)))\n",
        "    recall = recall_score(y_true, y_pred, average=\"macro\", labels=list(range(num_classes)))\n",
        "\n",
        "    return {\n",
        "        \"pixel_acc\": pixel_acc,\n",
        "        \"mean_iou\": mean_iou,\n",
        "        \"mean_dice\": mean_dice,\n",
        "        \"per_class_iou\": per_class_iou,\n",
        "        \"per_class_dice\": per_class_dice,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9eb2ff",
      "metadata": {
        "id": "ed9eb2ff"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def train_epoch(loader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss/len(loader)\n",
        "\n",
        "def eval_epoch(loader, model, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.numel()\n",
        "    return total_loss/len(loader), correct/total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "217f615b",
      "metadata": {
        "id": "217f615b",
        "outputId": "2d4a2481-18bc-4f7c-914f-6ba54138a147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 204389), started 12:24:58 ago. (Use '!kill 204389' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-60dfead7701f08e6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-60dfead7701f08e6\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "\n",
        "log_dir = \"runs/layerwise\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00769abd",
      "metadata": {
        "id": "00769abd"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = \"/home/mt/dataset/20230905_train_global_ps384.hdf5\"\n",
        "VAL_PATH = \"/home/mt/dataset/20230905_val_global_ps384.hdf5\"\n",
        "WEIGHT_DIR = \"weights/layerwise\"\n",
        "os.makedirs(WEIGHT_DIR, exist_ok=True)\n",
        "\n",
        "dataset_train = GlacierHDF5PatchDataset3(TRAIN_PATH, length=2000)\n",
        "dataset_val = GlacierHDF5PatchDataset3(VAL_PATH, length = 500)\n",
        "train_loader= DataLoader(dataset_train, batch_size=30, shuffle=True)\n",
        "dev_loader = DataLoader(dataset_val, batch_size=30, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SegFormer(num_classes=2, variant='mit_b0', drop_path_rate=0.2, in_chans=11).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=3e-6)\n",
        "EPOCHS = 35\n",
        "DEVICE = \"cuda\"\n",
        "NUM_CLASSES = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02af3452",
      "metadata": {
        "id": "02af3452",
        "outputId": "c59df63a-a88b-44c2-ee13-fe08b434c0da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total params: 679242\n",
            "Encoder params: 431543\n",
            "Decoder params: 247699\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Full model params\n",
        "model = SegFormer(num_classes=19, variant=\"mit_b0\", in_chans=3)\n",
        "print(\"Total params:\", count_parameters(model))\n",
        "\n",
        "# Encoder vs Decoder\n",
        "encoder_params = count_parameters(model.backbone)\n",
        "decoder_params = count_parameters(model.decoder)\n",
        "print(\"Encoder params:\", encoder_params)\n",
        "print(\"Decoder params:\", decoder_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fabbd96d",
      "metadata": {
        "id": "fabbd96d",
        "outputId": "97c92f68-02de-40e1-83b7-703fa87f951a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using device: cuda\n",
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "SegFormer                                          [1, 2, 256, 256]          --\n",
            "├─MixVisionTransformer: 1-1                        [1, 16, 128, 128]         --\n",
            "│    └─ModuleList: 2-1                             --                        --\n",
            "│    │    └─MiTStage: 3-1                          [1, 16, 128, 128]         5,795\n",
            "│    │    └─MiTStage: 3-2                          [1, 32, 64, 64]           19,057\n",
            "│    │    └─MiTStage: 3-3                          [1, 64, 32, 32]           84,416\n",
            "│    │    └─MiTStage: 3-4                          [1, 128, 16, 16]          322,432\n",
            "├─SegFormerHead: 1-2                               [1, 2, 128, 128]          --\n",
            "│    └─ModuleList: 2-2                             --                        --\n",
            "│    │    └─Sequential: 3-5                        [1, 128, 128, 128]        2,304\n",
            "│    │    └─Sequential: 3-6                        [1, 128, 64, 64]          4,352\n",
            "│    │    └─Sequential: 3-7                        [1, 128, 32, 32]          8,448\n",
            "│    │    └─Sequential: 3-8                        [1, 128, 16, 16]          16,640\n",
            "│    └─Sequential: 2-3                             [1, 128, 128, 128]        --\n",
            "│    │    └─Conv2d: 3-9                            [1, 128, 128, 128]        65,536\n",
            "│    │    └─BatchNorm2d: 3-10                      [1, 128, 128, 128]        256\n",
            "│    │    └─ReLU: 3-11                             [1, 128, 128, 128]        --\n",
            "│    │    └─Conv2d: 3-12                           [1, 128, 128, 128]        147,456\n",
            "│    │    └─BatchNorm2d: 3-13                      [1, 128, 128, 128]        256\n",
            "│    │    └─ReLU: 3-14                             [1, 128, 128, 128]        --\n",
            "│    └─Conv2d: 2-4                                 [1, 2, 128, 128]          258\n",
            "====================================================================================================\n",
            "Total params: 677,206\n",
            "Trainable params: 677,206\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 3.60\n",
            "====================================================================================================\n",
            "Input size (MB): 2.88\n",
            "Forward/backward pass size (MB): 149.19\n",
            "Params size (MB): 2.06\n",
            "Estimated Total Size (MB): 154.14\n",
            "====================================================================================================\n",
            "{'device': 'cuda', 'params': 514326.0, 'macs': 3764.912128, 'flops': 7529.824256, 'avg_inference_time_ms': 2.9012084007263184}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "from thop import profile\n",
        "from torchinfo import summary\n",
        "\n",
        "def analyze_model(model, input_size=(1, 11, 256, 256), device=None):\n",
        "    # --- Device selection ---\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"[INFO] Using device: {device}\")\n",
        "\n",
        "    # --- Move model and input to device ---\n",
        "    model = model.to(device)\n",
        "    dummy_input = torch.randn(*input_size).to(device)\n",
        "\n",
        "    # --- Verify device consistency ---\n",
        "    if next(model.parameters()).device != dummy_input.device:\n",
        "        raise RuntimeError(f\"Model and input device mismatch: model on {next(model.parameters()).device}, input on {dummy_input.device}\")\n",
        "\n",
        "    # --- FLOPs & MACs ---\n",
        "    try:\n",
        "        macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "        flops = 2 * macs  # 1 MAC = 2 FLOPs\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] thop profiling failed: {e}\")\n",
        "        macs, params, flops = 0, 0, 0  # Fallback values\n",
        "\n",
        "    # --- Torchinfo summary ---\n",
        "    try:\n",
        "        print(summary(model, input_size=input_size, device=device))\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] torchinfo summary failed: {e}\")\n",
        "\n",
        "    # --- Inference time ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Warm-up\n",
        "        for _ in range(5):\n",
        "            _ = model(dummy_input)\n",
        "        torch.cuda.synchronize() if device.startswith(\"cuda\") else None  # Sync for GPU\n",
        "        start = time.time()\n",
        "        for _ in range(20):\n",
        "            _ = model(dummy_input)\n",
        "        torch.cuda.synchronize() if device.startswith(\"cuda\") else None  # Sync for GPU\n",
        "        end = time.time()\n",
        "    avg_time = (end - start) / 20 * 1000  # Convert to ms/sample\n",
        "\n",
        "    return {\n",
        "        \"device\": device,\n",
        "        \"params\": params,\n",
        "        \"macs\": macs / 1e6,  # Convert to millions\n",
        "        \"flops\": flops / 1e6,  # Convert to millions\n",
        "        \"avg_inference_time_ms\": avg_time\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "try:\n",
        "    model = SegFormer(num_classes=2, in_chans=11)  # Ensure SegFormer is defined\n",
        "    stats = analyze_model(model, input_size=(1, 11, 256, 256), device=\"cuda\")\n",
        "    print(stats)\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Analysis failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b527c4d",
      "metadata": {
        "id": "0b527c4d"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Optional RMSNorm from mamba_ssm; fall back to LayerNorm if missing ---\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layer_norm import RMSNorm\n",
        "except Exception:\n",
        "    RMSNorm = None\n",
        "\n",
        "# --- REQUIRED: your local Mamba implementation exported from mamba_model.py ---\n",
        "from mamba_model import Mamba\n",
        "\n",
        "\n",
        "# ------------------------- Utils -------------------------\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob: float = 0.):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.drop_prob == 0. or not self.training:\n",
        "            return x\n",
        "        keep_prob = 1 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()\n",
        "        return x.div(keep_prob) * random_tensor\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ------------------------- Involution stem -------------------------\n",
        "class Involution2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Involution: location-specific, channel-agnostic kernel.\n",
        "    Paper: \"Involution: Inverting the Inherence of Convolution for Visual Recognition\" (CVPR 2021)\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, kernel_size=3, stride=1, reduction_ratio=4, groups=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.groups = groups\n",
        "\n",
        "        hidden = max(channels // reduction_ratio, 1)\n",
        "        self.reduce = nn.Conv2d(channels, hidden, 1)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        self.span = nn.Conv2d(hidden, groups * (kernel_size * kernel_size), 1)\n",
        "        self.sigma = nn.AvgPool2d(kernel_size=stride, stride=stride) if stride > 1 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        k, g = self.kernel_size, self.groups\n",
        "        assert C % g == 0, f\"Channels ({C}) must be divisible by groups ({g}).\"\n",
        "\n",
        "        # 1) Generate spatially-varying kernels on possibly downsampled features\n",
        "        x_k = self.sigma(x)                                   # (B, C, H', W')\n",
        "        K = self.span(self.act(self.reduce(x_k)))             # (B, g*k*k, H', W')\n",
        "        Hout, Wout = K.shape[2], K.shape[3]\n",
        "        K = K.view(B, g, k * k, Hout, Wout)                   # (B, g, k*k, H', W')\n",
        "\n",
        "        # 2) Unfold patches on original x with stride/padding so H',W' align\n",
        "        patches = F.unfold(x, kernel_size=k, dilation=1, padding=k // 2, stride=self.stride)\n",
        "        patches = patches.view(B, g, C // g, k * k, Hout, Wout)  # (B, g, Cg, k*k, H', W')\n",
        "\n",
        "        # 3) Channel-agnostic weighted sum within group\n",
        "        out = (patches * K.unsqueeze(2)).sum(dim=3)           # (B, g, Cg, H', W')\n",
        "        out = out.view(B, C, Hout, Wout)\n",
        "        return out\n",
        "\n",
        "\n",
        "class OverlapPatchEmbedInvo(nn.Module):\n",
        "    \"\"\"Involution downsample + 1x1 projection to embed_dim.\"\"\"\n",
        "    def __init__(self, in_chans=3, embed_dim=32, kernel_size=3, stride=2, padding=1,\n",
        "                 reduction_ratio=4, groups=1):\n",
        "        super().__init__()\n",
        "        self.invo = Involution2D(in_chans, kernel_size=kernel_size, stride=stride,\n",
        "                                 reduction_ratio=reduction_ratio, groups=groups)\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=1, stride=1, padding=0)\n",
        "        self.norm = nn.BatchNorm2d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.invo(x)     # downsample + spatially-varying mixing\n",
        "        x = self.proj(x)     # channel set to embed_dim\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ------------------------- Conv stem (fast option) -------------------------\n",
        "class OverlapPatchEmbedConv(nn.Module):\n",
        "    \"\"\"Standard conv downsample + BN + ReLU.\"\"\"\n",
        "    def __init__(self, in_chans=3, embed_dim=32, kernel_size=3, stride=2, padding=1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, embed_dim, kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(embed_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "# ------------------------- Mixer Block (Mamba) -------------------------\n",
        "class MambaBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Mamba SSM mixer on (B, N, C) with PreNorm + MLP and residuals.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0, drop_path=0.0, d_state=16, use_rmsnorm=True):\n",
        "        super().__init__()\n",
        "        Norm = RMSNorm if (use_rmsnorm and RMSNorm is not None) else nn.LayerNorm\n",
        "\n",
        "        self.norm1 = Norm(dim)\n",
        "        self.mamba = Mamba(d_model=dim, d_state=d_state)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "        self.norm2 = Norm(dim)\n",
        "        hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=hidden_dim, act_layer=nn.GELU, drop=drop)\n",
        "\n",
        "    def forward(self, x, H=None, W=None):\n",
        "        # x: (B, N, C)\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.mamba(x)\n",
        "        x = self.drop_path(x) + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop_path(x) + shortcut\n",
        "        return x\n",
        "\n",
        "# --- Placeholder for Transformer Block ---\n",
        "class TransformerBlockPlaceholder(nn.Module):\n",
        "    \"\"\"\n",
        "    Placeholder for a Transformer block.\n",
        "    Replace with actual Transformer block implementation if needed for ablation.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0, drop_path=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        # Placeholder for Attention\n",
        "        self.attn = nn.Identity()\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=hidden_dim, act_layer=nn.GELU, drop=drop)\n",
        "\n",
        "    def forward(self, x, H=None, W=None):\n",
        "        # x: (B, N, C)\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        # Apply placeholder attention\n",
        "        x = self.attn(x)\n",
        "        x = self.drop_path(x) + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop_path(x) + shortcut\n",
        "        return x\n",
        "\n",
        "\n",
        "# ------------------------- MiT Stage (stem + Mamba/Transformer blocks) -------------------------\n",
        "class MiTStage(nn.Module):\n",
        "    def __init__(self, in_chs, embed_dim, depth, drop_path_rates=None, use_involution=True, use_mamba=True):\n",
        "        super().__init__()\n",
        "        self.use_mamba = use_mamba\n",
        "\n",
        "        if use_involution:\n",
        "            self.patch_embed = OverlapPatchEmbedInvo(\n",
        "                in_chans=in_chs, embed_dim=embed_dim, kernel_size=3, stride=2, padding=1,\n",
        "                reduction_ratio=4, groups=1\n",
        "            )\n",
        "        else:\n",
        "            self.patch_embed = OverlapPatchEmbedConv(\n",
        "                in_chans=in_chs, embed_dim=embed_dim, kernel_size=3, stride=2, padding=1\n",
        "            )\n",
        "\n",
        "        # Choose block type based on use_mamba\n",
        "        block_type = MambaBlock if use_mamba else TransformerBlockPlaceholder # Assuming TransformerBlockPlaceholder is defined\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            block_type(\n",
        "                dim=embed_dim,\n",
        "                mlp_ratio=4.0,\n",
        "                drop=0.0,\n",
        "                drop_path=(drop_path_rates[i] if drop_path_rates is not None else 0.0),\n",
        "                # Only pass d_state and use_rmsnorm if using MambaBlock\n",
        "                **(dict(d_state=16, use_rmsnorm=True) if use_mamba else {})\n",
        "            )\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)              # (B, C, H', W')\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)     # (B, N, C)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, H, W)\n",
        "        x = x.transpose(1, 2).reshape(B, C, H, W)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ------------------------- Mix Vision Transformer (backbone) -------------------------\n",
        "class MixVisionTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Full MiT backbone (4 stages) returning multi-scale features:\n",
        "        [S1, S2, S3, S4] with strides [2, 4, 8, 16] wrt input.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_chans: int = 3,\n",
        "                 embed_dims = [32, 64, 160, 256],\n",
        "                 depths    = [2, 2, 2, 2],\n",
        "                 drop_path_rate: float = 0.0,\n",
        "                 use_invo_stages = (True, True, False, False),\n",
        "                 use_mamba_stages = (True, True, True, True)):\n",
        "        super().__init__()\n",
        "        assert len(embed_dims) == 4 and len(depths) == 4 and len(use_invo_stages) == 4 and len(use_mamba_stages) == 4\n",
        "\n",
        "        # Stochastic depth schedule across all blocks\n",
        "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "        cur = 0\n",
        "\n",
        "        self.stages = nn.ModuleList()\n",
        "        in_c = in_chans\n",
        "        for i in range(4):\n",
        "            d = depths[i]\n",
        "            dpr_slice = dp_rates[cur:cur + d]\n",
        "            cur += d\n",
        "\n",
        "            self.stages.append(\n",
        "                MiTStage(\n",
        "                    in_chs=in_c,\n",
        "                    embed_dim=embed_dims[i],\n",
        "                    depth=d,\n",
        "                    drop_path_rates=dpr_slice,\n",
        "                    use_involution=use_invo_stages[i],\n",
        "                    use_mamba=use_mamba_stages[i] # Pass use_mamba flag to MiTStage\n",
        "                )\n",
        "            )\n",
        "            in_c = embed_dims[i]\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "            features.append(x)\n",
        "        return features\n",
        "\n",
        "\n",
        "# ------------------------------- Decoder --------------------------------\n",
        "class SegFormerHead(nn.Module):\n",
        "    def __init__(self, in_channels: List[int], embed_dim=128, num_classes=19):\n",
        "        super().__init__()\n",
        "        self.proj_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(in_ch, embed_dim, 1, bias=False),\n",
        "                nn.BatchNorm2d(embed_dim),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "            for in_ch in in_channels\n",
        "        ])\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Conv2d(embed_dim * len(in_channels), embed_dim, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(embed_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(embed_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.classifier = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, features: List[torch.Tensor]):\n",
        "        # Upsample all features to the highest spatial resolution (stage 1)\n",
        "        target_h, target_w = features[0].shape[2:]\n",
        "        proj = []\n",
        "        for i, feat in enumerate(features):\n",
        "            x = self.proj_layers[i](feat)\n",
        "            if x.shape[2:] != (target_h, target_w):\n",
        "                x = F.interpolate(x, size=(target_h, target_w), mode='bilinear', align_corners=False)\n",
        "            proj.append(x)\n",
        "        x = torch.cat(proj, dim=1)\n",
        "        x = self.fuse(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# -------------------------- Full SegFormer Model -------------------------\n",
        "class SegFormer(nn.Module):\n",
        "    def __init__(self, num_classes=19, variant='mit_b0', pretrained=False,\n",
        "                 drop_path_rate=0.1, in_chans=3, use_invo_stages=(True, True, False, False),\n",
        "                 use_mamba_stages=(True, True, True, True), embed_dims=None, depths=None): # Added embed_dims and depths\n",
        "        super().__init__()\n",
        "        variants = {\n",
        "            'mit_b0': dict(embed_dims=[16, 32, 64, 128], depths=[1, 1, 1, 1]),\n",
        "            'mit_b1': dict(embed_dims=[64, 128, 320, 512], depths=[2, 2, 2, 2]),\n",
        "            'mit_b2': dict(embed_dims=[64, 128, 320, 512], depths=[3, 4, 6, 3]),\n",
        "            'mit_b3': dict(embed_dims=[64, 128, 320, 512], depths=[3, 4, 18, 3]),\n",
        "            'mit_b4': dict(embed_dims=[64, 128, 320, 512], depths=[3, 8, 27, 3]),\n",
        "            'mit_b5': dict(embed_dims=[64, 128, 320, 512], depths=[3, 6, 40, 3]),\n",
        "        }\n",
        "\n",
        "        if variant in variants:\n",
        "            cfg = variants[variant]\n",
        "            embed_dims = embed_dims or cfg['embed_dims']\n",
        "            depths = depths or cfg['depths']\n",
        "        elif embed_dims is None or depths is None:\n",
        "             raise ValueError(f'Unknown variant: {variant} and embed_dims or depths not provided')\n",
        "\n",
        "\n",
        "        self.backbone = MixVisionTransformer(\n",
        "            in_chans=in_chans,\n",
        "            embed_dims=embed_dims, # Pass embed_dims\n",
        "            depths=depths, # Pass depths\n",
        "            drop_path_rate=drop_path_rate,\n",
        "            use_invo_stages=use_invo_stages,\n",
        "            use_mamba_stages=use_mamba_stages # Pass use_mamba_stages to backbone\n",
        "        )\n",
        "        self.decoder = SegFormerHead(in_channels=embed_dims, embed_dim=128, num_classes=num_classes) # Pass embed_dims\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        if pretrained:\n",
        "            print('pretrained=True selected but no loader is implemented in this script.')\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if getattr(m, \"bias\", None) is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if getattr(m, \"bias\", None) is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, (nn.LayerNorm, RMSNorm)):\n",
        "            nn.init.ones_(m.weight)\n",
        "            if getattr(m, \"bias\", None) is not None: # Add this check\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        H, W = x.shape[2], x.shape[3]\n",
        "        feats = self.backbone(x)                       # [S1,S2,S3,S4]\n",
        "        out = self.decoder(feats)                      # logits at S1 resolution\n",
        "        out = F.interpolate(out, size=(H, W), mode='bilinear', align_corners=False)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53221a92",
      "metadata": {
        "id": "53221a92",
        "outputId": "c2812416-2247-4daa-d6ee-9d6fc065438c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running ablation: early_mamba_late_attn\n",
            "[INFO] Using device: cuda\n",
            "==============================================================================================================\n",
            "Layer (type:depth-idx)                                       Output Shape              Param #\n",
            "==============================================================================================================\n",
            "SegFormer                                                    [1, 2, 256, 256]          --\n",
            "├─MixVisionTransformer: 1-1                                  [1, 16, 128, 128]         --\n",
            "│    └─ModuleList: 2-1                                       --                        --\n",
            "│    │    └─MiTStage: 3-1                                    [1, 16, 128, 128]         5,795\n",
            "│    │    └─MiTStage: 3-2                                    [1, 32, 64, 64]           19,057\n",
            "│    │    └─MiTStage: 3-3                                    [1, 64, 32, 32]           51,904\n",
            "│    │    └─MiTStage: 3-4                                    [1, 128, 16, 16]          206,208\n",
            "├─SegFormerHead: 1-2                                         [1, 2, 128, 128]          --\n",
            "│    └─ModuleList: 2-2                                       --                        --\n",
            "│    │    └─Sequential: 3-5                                  [1, 128, 128, 128]        2,304\n",
            "│    │    └─Sequential: 3-6                                  [1, 128, 64, 64]          4,352\n",
            "│    │    └─Sequential: 3-7                                  [1, 128, 32, 32]          8,448\n",
            "│    │    └─Sequential: 3-8                                  [1, 128, 16, 16]          16,640\n",
            "│    └─Sequential: 2-3                                       [1, 128, 128, 128]        --\n",
            "│    │    └─Conv2d: 3-9                                      [1, 128, 128, 128]        65,536\n",
            "│    │    └─BatchNorm2d: 3-10                                [1, 128, 128, 128]        256\n",
            "│    │    └─ReLU: 3-11                                       [1, 128, 128, 128]        --\n",
            "│    │    └─Conv2d: 3-12                                     [1, 128, 128, 128]        147,456\n",
            "│    │    └─BatchNorm2d: 3-13                                [1, 128, 128, 128]        256\n",
            "│    │    └─ReLU: 3-14                                       [1, 128, 128, 128]        --\n",
            "│    └─Conv2d: 2-4                                           [1, 2, 128, 128]          258\n",
            "==============================================================================================================\n",
            "Total params: 528,470\n",
            "Trainable params: 528,470\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 3.60\n",
            "==============================================================================================================\n",
            "Input size (MB): 2.88\n",
            "Forward/backward pass size (MB): 149.19\n",
            "Params size (MB): 2.06\n",
            "Estimated Total Size (MB): 154.14\n",
            "==============================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:43<00:00,  1.55s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 1/35 | Train Loss 2.3127 | PixelAcc 0.6212 | mIoU 0.4488 | Dice 0.6189 | Precision 0.6812 | Recall 0.7027\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:44<00:00,  1.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 2/35 | Train Loss 1.7998 | PixelAcc 0.7135 | mIoU 0.5511 | Dice 0.7097 | Precision 0.7441 | Recall 0.7770\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:46<00:00,  1.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 3/35 | Train Loss 1.3415 | PixelAcc 0.7316 | mIoU 0.5656 | Dice 0.7200 | Precision 0.7392 | Recall 0.7966\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 4/35 | Train Loss 1.1693 | PixelAcc 0.7279 | mIoU 0.5673 | Dice 0.7228 | Precision 0.7538 | Recall 0.7950\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:44<00:00,  1.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 5/35 | Train Loss 0.8717 | PixelAcc 0.8333 | mIoU 0.7076 | Dice 0.8280 | Precision 0.8297 | Recall 0.8695\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:44<00:00,  1.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 6/35 | Train Loss 0.7550 | PixelAcc 0.8332 | mIoU 0.6992 | Dice 0.8212 | Precision 0.8142 | Recall 0.8701\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:41<00:00,  1.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 7/35 | Train Loss 0.6503 | PixelAcc 0.8644 | mIoU 0.7460 | Dice 0.8531 | Precision 0.8416 | Recall 0.8979\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 8/35 | Train Loss 0.5407 | PixelAcc 0.8743 | mIoU 0.7561 | Dice 0.8593 | Precision 0.8432 | Recall 0.9028\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:41<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 9/35 | Train Loss 0.5104 | PixelAcc 0.9108 | mIoU 0.8198 | Dice 0.9000 | Precision 0.8836 | Recall 0.9326\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 10/35 | Train Loss 0.4694 | PixelAcc 0.9012 | mIoU 0.8053 | Dice 0.8912 | Precision 0.8763 | Recall 0.9250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 11/35 | Train Loss 0.4315 | PixelAcc 0.9148 | mIoU 0.8278 | Dice 0.9050 | Precision 0.8891 | Recall 0.9362\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:40<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 12/35 | Train Loss 0.3746 | PixelAcc 0.9191 | mIoU 0.8391 | Dice 0.9119 | Precision 0.8987 | Recall 0.9387\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 13/35 | Train Loss 0.3613 | PixelAcc 0.9237 | mIoU 0.8456 | Dice 0.9157 | Precision 0.9016 | Recall 0.9416\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:43<00:00,  1.54s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 14/35 | Train Loss 0.3164 | PixelAcc 0.9439 | mIoU 0.8870 | Dice 0.9399 | Precision 0.9308 | Recall 0.9541\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:40<00:00,  1.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 15/35 | Train Loss 0.2846 | PixelAcc 0.9490 | mIoU 0.8934 | Dice 0.9434 | Precision 0.9321 | Recall 0.9597\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:41<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 16/35 | Train Loss 0.3267 | PixelAcc 0.9452 | mIoU 0.8827 | Dice 0.9372 | Precision 0.9229 | Recall 0.9581\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:41<00:00,  1.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 17/35 | Train Loss 0.2840 | PixelAcc 0.9533 | mIoU 0.8978 | Dice 0.9457 | Precision 0.9323 | Recall 0.9639\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:43<00:00,  1.54s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 18/35 | Train Loss 0.2316 | PixelAcc 0.9594 | mIoU 0.9085 | Dice 0.9517 | Precision 0.9382 | Recall 0.9692\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 19/35 | Train Loss 0.2499 | PixelAcc 0.9448 | mIoU 0.8842 | Dice 0.9382 | Precision 0.9261 | Recall 0.9555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 20/35 | Train Loss 0.2097 | PixelAcc 0.9604 | mIoU 0.9132 | Dice 0.9544 | Precision 0.9434 | Recall 0.9683\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:40<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 21/35 | Train Loss 0.2156 | PixelAcc 0.9638 | mIoU 0.9196 | Dice 0.9579 | Precision 0.9472 | Recall 0.9712\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 22/35 | Train Loss 0.2146 | PixelAcc 0.9530 | mIoU 0.8957 | Dice 0.9446 | Precision 0.9313 | Recall 0.9620\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 23/35 | Train Loss 0.2024 | PixelAcc 0.9513 | mIoU 0.9001 | Dice 0.9472 | Precision 0.9386 | Recall 0.9594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:39<00:00,  1.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 24/35 | Train Loss 0.2192 | PixelAcc 0.9443 | mIoU 0.8833 | Dice 0.9376 | Precision 0.9259 | Recall 0.9543\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:40<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 25/35 | Train Loss 0.1982 | PixelAcc 0.9696 | mIoU 0.9349 | Dice 0.9662 | Precision 0.9590 | Recall 0.9750\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 26/35 | Train Loss 0.1725 | PixelAcc 0.9501 | mIoU 0.8918 | Dice 0.9424 | Precision 0.9299 | Recall 0.9592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:40<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 27/35 | Train Loss 0.1880 | PixelAcc 0.9693 | mIoU 0.9323 | Dice 0.9648 | Precision 0.9559 | Recall 0.9755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 28/35 | Train Loss 0.1591 | PixelAcc 0.9701 | mIoU 0.9346 | Dice 0.9660 | Precision 0.9582 | Recall 0.9754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:41<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 29/35 | Train Loss 0.1362 | PixelAcc 0.9694 | mIoU 0.9327 | Dice 0.9651 | Precision 0.9570 | Recall 0.9747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:41<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 30/35 | Train Loss 0.1519 | PixelAcc 0.9728 | mIoU 0.9390 | Dice 0.9684 | Precision 0.9614 | Recall 0.9765\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 31/35 | Train Loss 0.1544 | PixelAcc 0.9625 | mIoU 0.9171 | Dice 0.9565 | Precision 0.9452 | Recall 0.9709\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:41<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 32/35 | Train Loss 0.1374 | PixelAcc 0.9697 | mIoU 0.9334 | Dice 0.9654 | Precision 0.9570 | Recall 0.9756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:40<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 33/35 | Train Loss 0.1521 | PixelAcc 0.9519 | mIoU 0.8978 | Dice 0.9458 | Precision 0.9332 | Recall 0.9638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:42<00:00,  1.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 34/35 | Train Loss 0.1535 | PixelAcc 0.9795 | mIoU 0.9524 | Dice 0.9755 | Precision 0.9697 | Recall 0.9819\n",
            "Saved best model for early_mamba_late_attn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:40<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variant early_mamba_late_attn | Epoch 35/35 | Train Loss 0.1228 | PixelAcc 0.9666 | mIoU 0.9283 | Dice 0.9626 | Precision 0.9569 | Recall 0.9693\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define ablation variants\n",
        "variants = [\n",
        "    {\"name\": \"early_mamba_late_attn\", \"use_mamba_stages\": (True, True, False, False),\n",
        "     \"embed_dims\": [16, 32, 64, 128], \"depths\": [1, 1, 1, 1]}\n",
        "]\n",
        "\n",
        "# Initialize dataset (fixed)\n",
        "dataset_train = GlacierHDF5PatchDataset3(TRAIN_PATH, length=2000)\n",
        "dataset_val = GlacierHDF5PatchDataset3(VAL_PATH, length=500)\n",
        "train_loader = DataLoader(dataset_train, batch_size=30, shuffle=True)\n",
        "dev_loader = DataLoader(dataset_val, batch_size=30, shuffle=False)\n",
        "\n",
        "# Run ablation experiments\n",
        "for variant in variants:\n",
        "    print(f\"\\nRunning ablation: {variant['name']}\")\n",
        "    log_dir = f\"runs/ablation_{variant['name']}\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    # Initialize model\n",
        "    model = SegFormer(\n",
        "        num_classes=2,\n",
        "        variant='mit_b0',\n",
        "        in_chans=11,\n",
        "        use_mamba_stages=variant['use_mamba_stages'],\n",
        "        use_invo_stages=(True, True, False, False),\n",
        "        embed_dims=variant['embed_dims'],\n",
        "        depths=variant['depths'],\n",
        "        drop_path_rate=0.2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Compute FLOPs/params\n",
        "    stats = analyze_model(model, input_size=(1, 11, 256, 256), device=DEVICE)\n",
        "    writer.add_scalar(\"Params/total\", stats[\"params\"], 0)\n",
        "    writer.add_scalar(\"FLOPs/total\", stats[\"flops\"], 0)\n",
        "    writer.add_scalar(\"InferenceTime_ms\", stats[\"avg_inference_time_ms\"], 0)\n",
        "\n",
        "    # Train\n",
        "    optimizer = AdamW(model.parameters(), lr=3e-6)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_val_iou = 0.0\n",
        "    weight_dir = os.path.join(WEIGHT_DIR, variant['name'])\n",
        "    os.makedirs(weight_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train_epoch(train_loader, model, criterion, optimizer, DEVICE)\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for x, y in dev_loader:\n",
        "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "                outputs = model(x)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        y_true = np.concatenate(all_labels)\n",
        "        y_pred = np.concatenate(all_preds)\n",
        "        metrics = segmentation_metrics(y_true, y_pred, NUM_CLASSES)\n",
        "\n",
        "        print(f\"Variant {variant['name']} | Epoch {epoch+1}/{EPOCHS} | \"\n",
        "              f\"Train Loss {train_loss:.4f} | \"\n",
        "              f\"PixelAcc {metrics['pixel_acc']:.4f} | \"\n",
        "              f\"mIoU {metrics['mean_iou']:.4f} | \"\n",
        "              f\"Dice {metrics['mean_dice']:.4f} | \"\n",
        "              f\"Precision {metrics['precision']:.4f} | \"\n",
        "              f\"Recall {metrics['recall']:.4f}\")\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "        writer.add_scalar(\"Acc/pixel\", metrics[\"pixel_acc\"], epoch)\n",
        "        writer.add_scalar(\"IoU/mean\", metrics[\"mean_iou\"], epoch)\n",
        "        writer.add_scalar(\"Dice/mean\", metrics[\"mean_dice\"], epoch)\n",
        "        writer.add_scalar(\"Precision/mean\", metrics[\"precision\"], epoch)\n",
        "        writer.add_scalar(\"Recall/mean\", metrics[\"recall\"], epoch)\n",
        "\n",
        "        ckpt_path = os.path.join(weight_dir, f\"epoch_{epoch+1:03d}.pth\")\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        if metrics[\"mean_iou\"] > best_val_iou:\n",
        "            best_val_iou = metrics[\"mean_iou\"]\n",
        "            best_ckpt_path = os.path.join(weight_dir, \"best_model.pth\")\n",
        "            torch.save(model.state_dict(), best_ckpt_path)\n",
        "            print(f\"Saved best model for {variant['name']}\")\n",
        "\n",
        "    dataset_train.close()\n",
        "    dataset_val.close()\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ea93a4",
      "metadata": {
        "id": "e1ea93a4",
        "outputId": "b2dc0583-1eb3-4e2d-8222-7529deca8b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "\n",
            "Evaluating variant: default_mamba\n",
            "Loaded cleaned weights from: weights/layerwise/default_mamba/best_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferring default_mamba:   0%|          | 0/75 [00:00<?, ?it/s]/tmp/ipykernel_204280/2441580367.py:91: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
            "  plt.tight_layout()\n",
            "Inferring default_mamba: 100%|██████████| 75/75 [01:27<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved metrics to ablation_viz/default_mamba/metrics.json\n",
            "\n",
            "Evaluating variant: stage1_deep\n",
            "Loaded cleaned weights from: weights/layerwise/stage1_deep/best_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferring stage1_deep: 100%|██████████| 75/75 [01:09<00:00,  1.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved metrics to ablation_viz/stage1_deep/metrics.json\n",
            "\n",
            "Evaluating variant: stage4_deep\n",
            "Loaded cleaned weights from: weights/layerwise/stage4_deep/best_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferring stage4_deep: 100%|██████████| 75/75 [01:10<00:00,  1.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved metrics to ablation_viz/stage4_deep/metrics.json\n",
            "\n",
            "Evaluating variant: stage4_skip\n",
            "Loaded cleaned weights from: weights/layerwise/stage4_skip/best_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferring stage4_skip: 100%|██████████| 75/75 [01:08<00:00,  1.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved metrics to ablation_viz/stage4_skip/metrics.json\n",
            "\n",
            "Evaluating variant: early_mamba_late_attn\n",
            "Loaded cleaned weights from: weights/layerwise/early_mamba_late_attn/best_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferring early_mamba_late_attn: 100%|██████████| 75/75 [01:09<00:00,  1.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved metrics to ablation_viz/early_mamba_late_attn/metrics.json\n",
            "\n",
            "Evaluating variant: stage1_light\n",
            "Loaded cleaned weights from: weights/layerwise/stage1_light/best_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferring stage1_light: 100%|██████████| 75/75 [01:10<00:00,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved metrics to ablation_viz/stage1_light/metrics.json\n",
            "\n",
            "--- Summary of all variants ---\n",
            "Variant: default_mamba\n",
            "  PixelAcc: 0.6014\n",
            "  mIoU: 0.4297\n",
            "  Dice: 0.6009\n",
            "--------------------\n",
            "Variant: stage1_deep\n",
            "  PixelAcc: 0.9547\n",
            "  mIoU: 0.8998\n",
            "  Dice: 0.9468\n",
            "--------------------\n",
            "Variant: stage4_deep\n",
            "  PixelAcc: 0.9041\n",
            "  mIoU: 0.8089\n",
            "  Dice: 0.8933\n",
            "--------------------\n",
            "Variant: stage4_skip\n",
            "  PixelAcc: 0.9599\n",
            "  mIoU: 0.9110\n",
            "  Dice: 0.9531\n",
            "--------------------\n",
            "Variant: early_mamba_late_attn\n",
            "  PixelAcc: 0.9807\n",
            "  mIoU: 0.9574\n",
            "  Dice: 0.9782\n",
            "--------------------\n",
            "Variant: stage1_light\n",
            "  PixelAcc: 0.9218\n",
            "  mIoU: 0.8391\n",
            "  Dice: 0.9117\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'all_variants_metrics' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 341\u001b[39m\n\u001b[32m    339\u001b[39m     writer = csv.writer(f)\n\u001b[32m    340\u001b[39m     writer.writerow([\u001b[33m\"\u001b[39m\u001b[33mVariant\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPixelAcc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmIoU\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDice\u001b[39m\u001b[33m\"\u001b[39m])  \u001b[38;5;66;03m# header\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, metrics \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_variants_metrics\u001b[49m.items():\n\u001b[32m    342\u001b[39m         writer.writerow([\n\u001b[32m    343\u001b[39m             name,\n\u001b[32m    344\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[33m'\u001b[39m\u001b[33mpixel_acc\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    345\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[33m'\u001b[39m\u001b[33mmean_iou\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    346\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[33m'\u001b[39m\u001b[33mmean_dice\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    347\u001b[39m         ])\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# Write JSON\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'all_variants_metrics' is not defined"
          ]
        }
      ],
      "source": [
        "# viz_predictions_with_metrics.py\n",
        "\n",
        "import os, json, random\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ====== USER SETTINGS ======\n",
        "TEST_PATH   = \"/home/mt/dataset/20230905_test_global_ps384.hdf5\"\n",
        "SAVE_BASE_DIR    = \"ablation_viz\"\n",
        "NUM_IMAGES  = 300\n",
        "BATCH_SIZE  = 4\n",
        "IN_CHANS    = 11\n",
        "NUM_CLASSES = 2\n",
        "# VARIANT     = 'mit_b0' # This will be set per variant\n",
        "DROP_PATH   = 0.2 # Keep consistent with training\n",
        "IGNORE_INDEX = None   # e.g., 255 if your labels have an ignore id; else None\n",
        "WEIGHT_BASE_DIR = \"weights/layerwise\" # Base directory where variant weights are saved\n",
        "\n",
        "# Define ablation variants (should match the training script)\n",
        "variants = [\n",
        "    {\"name\": \"default_mamba\", \"use_mamba_stages\": (True, True, True, True),\n",
        "     \"embed_dims\": [16, 32, 64, 128], \"depths\": [1, 1, 1, 1]},\n",
        "    {\"name\": \"stage1_deep\", \"use_mamba_stages\": (True, True, True, True),\n",
        "     \"embed_dims\": [16, 32, 64, 128], \"depths\": [2, 1, 1, 1]},\n",
        "    {\"name\": \"stage4_deep\", \"use_mamba_stages\": (True, True, True, True),\n",
        "     \"embed_dims\": [16, 32, 64, 128], \"depths\": [1, 1, 1, 2]},\n",
        "    {\"name\": \"stage4_skip\", \"use_mamba_stages\": (True, True, True, True),\n",
        "     \"embed_dims\": [16, 32, 64, 128], \"depths\": [1, 1, 1, 0]},\n",
        "    {\"name\": \"early_mamba_late_attn\", \"use_mamba_stages\": (True, True, False, False),\n",
        "     \"embed_dims\": [16, 32, 64, 128], \"depths\": [1, 1, 1, 1]},\n",
        "    {\"name\": \"stage1_light\", \"use_mamba_stages\": (True, True, True, True),\n",
        "     \"embed_dims\": [8, 32, 64, 128], \"depths\": [1, 1, 1, 1]},\n",
        "]\n",
        "\n",
        "# ====== VIZ UTILS ======\n",
        "def to_pseudo_rgb(x: torch.Tensor) -> np.ndarray:\n",
        "    x = x.float()\n",
        "    C, H, W = x.shape\n",
        "    def norm(ch):\n",
        "        ch = ch.clone()\n",
        "        vmin, vmax = torch.min(ch), torch.max(ch)\n",
        "        if float(vmax - vmin) < 1e-8:\n",
        "            return torch.zeros_like(ch)\n",
        "        return (ch - vmin) / (vmax - vmin)\n",
        "    if C >= 3:\n",
        "        r = norm(x[0]); g = norm(x[1]); b = norm(x[2])\n",
        "    elif C == 2:\n",
        "        r = norm(x[0]); g = norm(x[1]); b = torch.zeros_like(r)\n",
        "    else:\n",
        "        r = g = b = norm(x[0])\n",
        "    return torch.stack([r, g, b], dim=-1).cpu().numpy().astype(np.float32)\n",
        "\n",
        "def colorize_mask(mask: np.ndarray, class_colors=None) -> np.ndarray:\n",
        "    if class_colors is None:\n",
        "        class_colors = [(30, 30, 30), (0, 200, 255)]\n",
        "    h, w = mask.shape\n",
        "    out = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    max_id = int(mask.max()) if mask.size else 0\n",
        "    while len(class_colors) <= max_id:\n",
        "        class_colors.append(tuple(np.random.randint(0, 255, size=3).tolist()))\n",
        "    for cid, color in enumerate(class_colors):\n",
        "        out[mask == cid] = color\n",
        "    return out\n",
        "\n",
        "def overlay_image(image_rgb: np.ndarray, mask_rgb: np.ndarray, alpha=0.45) -> np.ndarray:\n",
        "    img = image_rgb.copy()\n",
        "    m = (mask_rgb.astype(np.float32) / 255.0)\n",
        "    out = (1 - alpha) * img + alpha * m\n",
        "    return np.clip(out, 0, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def save_grid(rgb, y_rgb, p_rgb, over_gt, over_pred, out_path):\n",
        "    fig = plt.figure(figsize=(12, 6))\n",
        "    gs = fig.add_gridspec(2, 3, hspace=0.15, wspace=0.05)\n",
        "    ax1 = fig.add_subplot(gs[:, 0])\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax3 = fig.add_subplot(gs[1, 1])\n",
        "    ax4 = fig.add_subplot(gs[0, 2])\n",
        "    ax5 = fig.add_subplot(gs[1, 2])\n",
        "\n",
        "    ax1.imshow(rgb);       ax1.set_title(\"Pseudo-RGB\");  ax1.axis('off')\n",
        "    ax2.imshow(y_rgb);     ax2.set_title(\"GT Mask\");     ax2.axis('off')\n",
        "    ax3.imshow(p_rgb);     ax3.set_title(\"Pred Mask\");   ax3.axis('off')\n",
        "    ax4.imshow(over_gt);   ax4.set_title(\"Overlay GT\");  ax4.axis('off')\n",
        "    ax5.imshow(over_pred); ax5.set_title(\"Overlay Pred\");ax5.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    plt.savefig(out_path, dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "# ====== DATASET (3-modal, 11-chan) ======\n",
        "PATCH_SIZE = 256\n",
        "TARGET_SIZE = (256, 256)\n",
        "LABEL_KEY = 'outlines'\n",
        "MODALITIES_3 = ['dem', 'optical', 'bright_dark_outlines']  # 2 + 6 + 3 = 11 chans\n",
        "\n",
        "def normalize(arr):\n",
        "    arr = arr.astype(np.float32)\n",
        "    return (arr - arr.mean()) / (arr.std() + 1e-5)\n",
        "\n",
        "# Augmentation not needed for evaluation\n",
        "# def augment_patch(img, label):\n",
        "#     if random.random() < 0.5:\n",
        "#         img = np.flip(img, axis=0); label = np.flip(label, axis=0)\n",
        "#     if random.random() < 0.5:\n",
        "#         img = np.flip(img, axis=1); label = np.flip(label, axis=1)\n",
        "#     return img, label\n",
        "\n",
        "class GlacierHDF5PatchDataset3(torch.utils.data.Dataset):\n",
        "    def __init__(self, hdf5_file_path, patch_size=PATCH_SIZE, target_size=TARGET_SIZE, length=2000):\n",
        "        self.hdf5 = h5py.File(hdf5_file_path, 'r')\n",
        "        self.tiles = [name for name in self.hdf5.keys() if all(m in self.hdf5[name] for m in MODALITIES_3)]\n",
        "        self.patch_size = patch_size\n",
        "        self.target_size = target_size\n",
        "        self.length = length\n",
        "        # Pre-select random indices for the fixed number of images\n",
        "        self.selected_indices = [self._get_random_patch_coords() for _ in range(length)]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def _get_random_patch_coords(self):\n",
        "        tile_name = random.choice(self.tiles)\n",
        "        tile = self.hdf5[tile_name]\n",
        "        h, w = tile[MODALITIES_3[0]].shape[:2]\n",
        "        y = random.randint(0, h - self.patch_size)\n",
        "        x = random.randint(0, w - self.patch_size)\n",
        "        return tile_name, y, x\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tile_name, y, x = self.selected_indices[idx]\n",
        "        tile = self.hdf5[tile_name]\n",
        "\n",
        "        input_channels = []\n",
        "        for key in MODALITIES_3:\n",
        "            arr = tile[key][y:y+self.patch_size, x:x+self.patch_size, :]\n",
        "            arr = normalize(arr)\n",
        "            input_channels.append(arr)\n",
        "        input_patch = np.concatenate(input_channels, axis=2)  # (H,W,11)\n",
        "\n",
        "        label = tile[LABEL_KEY][y:y+self.patch_size, x:x+self.patch_size]\n",
        "        if label.ndim == 3:\n",
        "            label = label[:, :, 0] if label.shape[2] == 1 else np.argmax(label, axis=2)\n",
        "\n",
        "        # Augmentation is not needed for evaluation\n",
        "        # input_patch, label = augment_patch(input_patch, label)\n",
        "\n",
        "        input_tensor = torch.tensor(np.ascontiguousarray(input_patch)).permute(2, 0, 1).float()  # (C,H,W)\n",
        "        label_tensor = torch.tensor(np.ascontiguousarray(label), dtype=torch.long)               # (H,W)\n",
        "        return input_tensor, label_tensor\n",
        "\n",
        "    def close(self):\n",
        "        self.hdf5.close()\n",
        "\n",
        "# ====== METRICS ======\n",
        "def _fast_confusion_matrix(pred, target, num_classes, ignore_index=None):\n",
        "    \"\"\"\n",
        "    pred, target: 1D arrays with same length (flattened), type int\n",
        "    Returns (K,K) confusion matrix where rows = GT, cols = Pred.\n",
        "    \"\"\"\n",
        "    if ignore_index is not None:\n",
        "        mask = target != ignore_index\n",
        "        pred = pred[mask]\n",
        "        target = target[mask]\n",
        "    # remove out-of-range labels just in case\n",
        "    mask = (target >= 0) & (target < num_classes)\n",
        "    pred = pred[mask]; target = target[mask]\n",
        "    cm = np.bincount(num_classes * target + pred, minlength=num_classes**2).reshape(num_classes, num_classes)\n",
        "    return cm.astype(np.int64)\n",
        "\n",
        "def compute_metrics_from_confmat(conf_mat):\n",
        "    \"\"\"\n",
        "    conf_mat: (K,K), rows=GT, cols=Pred\n",
        "    Returns dict with pixel_acc, mean_iou, mean_dice, per-class stats, etc.\n",
        "    \"\"\"\n",
        "    K = conf_mat.shape[0]\n",
        "    tp = np.diag(conf_mat).astype(np.float64)\n",
        "    fp = conf_mat.sum(0) - tp\n",
        "    fn = conf_mat.sum(1) - tp\n",
        "    denom_iou  = tp + fp + fn\n",
        "    denom_dice = 2*tp + fp + fn\n",
        "\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        iou  = np.where(denom_iou  > 0, tp / denom_iou, 0.0)\n",
        "        dice = np.where(denom_dice > 0, 2*tp / denom_dice, 0.0)\n",
        "\n",
        "    total = conf_mat.sum()\n",
        "    pixel_acc = float(tp.sum() / total) if total > 0 else 0.0\n",
        "    mean_iou  = float(np.mean(iou))\n",
        "    mean_dice = float(np.mean(dice))\n",
        "\n",
        "    return {\n",
        "        \"pixel_acc\": pixel_acc,\n",
        "        \"mean_iou\":  mean_iou,\n",
        "        \"mean_dice\": mean_dice,\n",
        "        \"per_class_iou\":  iou.tolist(),\n",
        "        \"per_class_dice\": dice.tolist(),\n",
        "        \"confusion_matrix\": conf_mat.tolist(),\n",
        "        \"total_pixels\": int(total),\n",
        "    }\n",
        "\n",
        "# ====== MAIN ======\n",
        "def main():\n",
        "    # ---- Device ----\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Device:\", device)\n",
        "\n",
        "    # ---- Dataset & Loader ----\n",
        "    dataset = GlacierHDF5PatchDataset3(TEST_PATH, length=NUM_IMAGES)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False, # No shuffle for evaluation\n",
        "        num_workers=0,   # h5py-safe\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    all_variants_metrics = {} # Dictionary to store metrics for each variant\n",
        "\n",
        "    for variant in variants:\n",
        "        variant_name = variant['name']\n",
        "        print(f\"\\nEvaluating variant: {variant_name}\")\n",
        "\n",
        "        # ---- Model ----\n",
        "        model = SegFormer(num_classes=NUM_CLASSES,\n",
        "                          variant='mit_b0', # Keep base variant name\n",
        "                          drop_path_rate=DROP_PATH,\n",
        "                          in_chans=IN_CHANS,\n",
        "                          use_mamba_stages=variant['use_mamba_stages'],\n",
        "                          use_invo_stages=(True, True, False, False), # Assuming consistent stem setup as in training\n",
        "                          embed_dims=variant['embed_dims'],\n",
        "                          depths=variant['depths']\n",
        "                          ).to(device)\n",
        "\n",
        "        checkpoint_path = os.path.join(WEIGHT_BASE_DIR, variant_name, \"best_model.pth\")\n",
        "        assert os.path.isfile(checkpoint_path), f\"Checkpoint not found for {variant_name}: {checkpoint_path}\"\n",
        "\n",
        "        def clean_state_dict(state):\n",
        "            return {k: v for k, v in state.items() if \"total_ops\" not in k and \"total_params\" not in k}\n",
        "\n",
        "        state = torch.load(checkpoint_path, map_location=device)\n",
        "        state = clean_state_dict(state)\n",
        "        model.load_state_dict(state, strict=False)\n",
        "        model.eval()\n",
        "        print(f\"Loaded cleaned weights from: {checkpoint_path}\")\n",
        "\n",
        "\n",
        "        save_dir = os.path.join(SAVE_BASE_DIR, variant_name)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # ---- Inference, Save viz, and Accumulate Confusion Matrix ----\n",
        "        saved = 0\n",
        "        conf_mat = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=np.int64)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for bi, (xb, yb) in enumerate(tqdm(loader, desc=f\"Inferring {variant_name}\")): # Added tqdm for progress\n",
        "                xb = xb.to(device)                 # [B,C,H,W]\n",
        "                yb = yb.to(device).long()          # [B,H,W]\n",
        "\n",
        "                logits = model(xb)                 # [B,K,H,W]  (K=NUM_CLASSES)\n",
        "                if logits.dim() == 4 and logits.size(1) > 1:\n",
        "                    preds = torch.argmax(logits, dim=1)  # [B,H,W]\n",
        "                else:\n",
        "                    preds = (logits.squeeze(1) > 0).long()\n",
        "\n",
        "                # ---- Metrics accumulation ----\n",
        "                # Flatten and move to CPU numpy\n",
        "                y_flat = yb.view(-1).detach().cpu().numpy()\n",
        "                p_flat = preds.view(-1).detach().cpu().numpy()\n",
        "                conf_mat += _fast_confusion_matrix(\n",
        "                    p_flat, y_flat, num_classes=NUM_CLASSES, ignore_index=IGNORE_INDEX\n",
        "                )\n",
        "\n",
        "                # ---- Visualization ----\n",
        "                # Only save visualizations for a subset to avoid generating too many images\n",
        "                if saved < NUM_IMAGES:\n",
        "                     for i in range(xb.size(0)):\n",
        "                        if saved < NUM_IMAGES:\n",
        "                            x  = xb[i].detach().cpu()           # (C,H,W)\n",
        "                            y  = yb[i].detach().cpu().numpy()   # (H,W)\n",
        "                            p  = preds[i].detach().cpu().numpy()\n",
        "\n",
        "                            rgb    = to_pseudo_rgb(x)\n",
        "                            y_rgb  = colorize_mask(y)\n",
        "                            p_rgb  = colorize_mask(p)\n",
        "                            over_y = overlay_image(rgb, y_rgb, alpha=0.45)\n",
        "                            over_p = overlay_image(rgb, p_rgb, alpha=0.45)\n",
        "\n",
        "                            out_path = os.path.join(save_dir, f\"pred_{saved:04d}.png\")\n",
        "                            save_grid(rgb, y_rgb, p_rgb, over_y, over_p, out_path)\n",
        "                            # print(f\"Saved {out_path}\") # Avoid excessive printing during loop\n",
        "                            saved += 1\n",
        "                        else:\n",
        "                            break # Stop saving visualizations once NUM_IMAGES is reached\n",
        "\n",
        "\n",
        "        # ---- Compute and Save metrics for the current variant ----\n",
        "        metrics = compute_metrics_from_confmat(conf_mat)\n",
        "        metrics_path = os.path.join(save_dir, \"metrics.json\")\n",
        "        with open(metrics_path, \"w\") as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "        print(f\"Saved metrics to {metrics_path}\")\n",
        "\n",
        "        all_variants_metrics[variant_name] = metrics # Store metrics\n",
        "\n",
        "    # ---- Close dataset and print summary of all variants ----\n",
        "    if hasattr(dataset, \"close\"):\n",
        "        dataset.close()\n",
        "\n",
        "    print(\"\\n--- Summary of all variants ---\")\n",
        "    for name, metrics in all_variants_metrics.items():\n",
        "        print(f\"Variant: {name}\")\n",
        "        print(f\"  PixelAcc: {metrics['pixel_acc']:.4f}\")\n",
        "        print(f\"  mIoU: {metrics['mean_iou']:.4f}\")\n",
        "        print(f\"  Dice: {metrics['mean_dice']:.4f}\")\n",
        "        # print(f\"  Per-class IoU: {metrics['per_class_iou']}\")\n",
        "        # print(f\"  Per-class Dice: {metrics['per_class_dice']}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    import csv\n",
        "\n",
        "    # ---- Save combined metrics (CSV + JSON) ----\n",
        "    summary_csv = os.path.join(SAVE_BASE_DIR, \"all_variants_summary.csv\")\n",
        "    summary_json = os.path.join(SAVE_BASE_DIR, \"all_variants_summary.json\")\n",
        "\n",
        "    os.makedirs(SAVE_BASE_DIR, exist_ok=True)\n",
        "\n",
        "    # Write CSV\n",
        "    with open(summary_csv, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Variant\", \"PixelAcc\", \"mIoU\", \"Dice\"])  # header\n",
        "        for name, metrics in all_variants_metrics.items():\n",
        "            writer.writerow([\n",
        "                name,\n",
        "                f\"{metrics['pixel_acc']:.4f}\",\n",
        "                f\"{metrics['mean_iou']:.4f}\",\n",
        "                f\"{metrics['mean_dice']:.4f}\"\n",
        "            ])\n",
        "\n",
        "    # Write JSON\n",
        "    with open(summary_json, \"w\") as f:\n",
        "        json.dump(all_variants_metrics, f, indent=2)\n",
        "\n",
        "    print(f\"\\nSaved combined results to:\\n  {summary_csv}\\n  {summary_json}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53dc2db2",
      "metadata": {
        "id": "53dc2db2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
